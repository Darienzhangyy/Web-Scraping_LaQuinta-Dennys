---
title: "hw2"
output: pdf_document
---


1. get_lq.R

   We download information of LaQuinta through steps as below:

A. Create a directory named 'data/lq/' to store links of LaQuinta's websites;

B. Create a function named Scrape_list such that it: 
   
   1. Match US state names from csv file to US state abbreviations on website to identify American hotels;
   
   2. Extract country/state abbreviations and replace NA with "NA;
   
   3. Extract the index from `rows` corresponding to the first entry after the US label;
   
   4. Extract the index from `rows` corresponding to the last entry before the Mexico label;
   
   5. Extract the indexes from `rows` corresponding to the state abbreviations;
   
   6. Extract the indexes from `rows` immediately following the state abbreviations;
   
   7. Extract the indexes from `rows` corresponding to all American hotels;
   
   8. Extract the links from the nodes indexed by `state_rows`
   
   9. Then the function finally returns "links".


C. Create a function 'html_download' such that it download a webpage and saved it to disk as an html file and direct the computer to rest for a quarter of a second;

D. Apply 'Scrape_list' function to LaQuinta websites that is previously stored, then apply  'html_download' function to save each link that Scrape_list function returns.







parse_lq.R:

We parse laQuinta's data through steps as below:

A. Read in the html files to a list and remove the duplicate link if present;

B. Create a function called scrape_hotel() that it takes input as 'page', the full html file for a particular La Quinta hotel's webpage. And it returns

Output: info, a named list containing 8 character vectors:
       
       name, the name of the hotel;
        
       address, the address of the hotel;
       
       phone, the phone number of the hotel;
       
       fax, the fax number of the hotel;
       
       latitude, the latitude of the hotel location;
       
       longitude, the longitude of the hotel location;
       
       amenities, the features of the hotel and the room;
       
       details, the hotel details (# rooms, floors, etc.).
       


How we construct scrape_hotel():

1. Parse the html;

2. Extract the hotel name;

3. Extract the raw address, phone number, and fax number into a vector, in that order;

4. Extract and clean the address, phone number, and fax number from the contact vector;

5. Extract the latitude and longitude of the hotel and output as character vector;

6. Extract the hotel and room features and output as character vector;

7. Extract the hotel details from lists and output as named character vector;

8. Extract the relevant information from the hotel webpages into a data frame;



C. After obtaining  the relative information, check hotel names and addresses by visual inspection;

D. Check hotel phone numbers by adherence to "1-xxx-xxx-xxxx" pattern and correct as needed;

E. Replacing " " with "-" corrects all flaws;

F. Check hotel fax numbers by adherence to "1-xxx-xxx-xxxx" pattern and correct as needed;

G. Replacing " " with "-" corrects most - but not all - flaws;

H. Inserting "-" in the last seven digits corrects one more flaw;

I. Trimming leading zeroes corrects one more flaw;

J. Three remaining flaws involve missing data; convert to NAs;

K. Check hotel names and addresses by summary inspection and correct as needed;

M. Check hotel amenities and details by summary inspection and correct as needed;

N. Check if the hotel details are not consistent;

O. Check the inconsistent entries by visual inspection;

P. Define and call a function to standardize hotel details;

Q. Write the data frame to disk.




get_dannys.R


We downloaded xml data from the Where2GetIt API through steps as follows:

1. Create a variable named 'to_get' to read the file 'dannys_coords.csv' and create a directory called 'data/dennys/' in order to store API information later;

2. Create a function named query_api that has following features:

  a. Input: coordinate, a data frame row containing a longitude, a latitude, and a search radius;
     
            key (default=api_key), a valid API key allowing data scraping;
            
            limit (default=1000), an upper bound for the number of search results returned.

  b. Output: Nothing; an XML file containing the results of the API query is downloaded to disk.
 

3. Run query_api on the rows of the to_get data frame to download XML files containing Dennys locations.



parse_dannys.R


We parse danny's data with following steps:

1.Read in the xml files to a list;

2.Create a logical vector for subsetting the unique store ID numbers;

3.Extract the results nodes for the unique Dennys locations;

4.Create a function named scrape_tag() such that it has following features:

   a.Input: tag, a particular xml tag whose value is to be extracted;
 
         result, a particular result node.
   
   b.Output: the value of the xml tag.

5. Create a function named scrape_dennys() such that it has following features:

   a.Input: result, a particular result node.
    
   b.Output: a character vector with five elements:
            
            (1) clientkey, a unique store ID;
            
            (2) address, the address of the Dennys location;
            
            (3) phone, the phone number of the Dennys location;
            
            (4) latitude, the latitude of the Dennys location;
      
            (5) longitude, the longitude of the Dennys location.
           
6. Extract the relevant information from the search results into a data frame;

7. Subset the data frame to include only US locations (those with a state abbreviation and ZIP code);

8. Write the data frame to disk and save it as 'dennys.Rdata'.
